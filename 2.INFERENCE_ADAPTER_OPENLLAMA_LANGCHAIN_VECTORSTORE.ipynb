{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"f24e43907ddf4843b1882e2ebced166e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8d8649dd03e54f3281ebe45d3969d6f5","IPY_MODEL_6e736b806e684fc5a1c79dea6195c487","IPY_MODEL_9a995ed9556f4682bf492e5debc92e54"],"layout":"IPY_MODEL_2148ff4c7d6a4ab48bcbe0cce7311fed"}},"8d8649dd03e54f3281ebe45d3969d6f5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0c462a91ef2141f792e4cfabff176049","placeholder":"​","style":"IPY_MODEL_be5ccd3fa5394cc6b2d1693a31e917f5","value":"Downloading (…)/adapter_config.json: 100%"}},"6e736b806e684fc5a1c79dea6195c487":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_24e440d5d3ad4e28ae0152dd7f66d785","max":595,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cd8be2fecc5f4ada96e00a5ad6e9aafb","value":595}},"9a995ed9556f4682bf492e5debc92e54":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_44aeb868743e466ea90e1d3748f878bd","placeholder":"​","style":"IPY_MODEL_70f3facc0bf344d788d1eca8fddcb08d","value":" 595/595 [00:00&lt;00:00, 20.0kB/s]"}},"2148ff4c7d6a4ab48bcbe0cce7311fed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0c462a91ef2141f792e4cfabff176049":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"be5ccd3fa5394cc6b2d1693a31e917f5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"24e440d5d3ad4e28ae0152dd7f66d785":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cd8be2fecc5f4ada96e00a5ad6e9aafb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"44aeb868743e466ea90e1d3748f878bd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"70f3facc0bf344d788d1eca8fddcb08d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e68425f516ea474993dc28e50dd2818f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_aed4fd1d25c54f89a3b9385daf8ed643","IPY_MODEL_d82ae21155e24950bf2f200832869c20","IPY_MODEL_2b360333c9da405ea70cd559c2bd0fe4"],"layout":"IPY_MODEL_3b75ab9513b347daa0bdb01550d386f4"}},"aed4fd1d25c54f89a3b9385daf8ed643":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4e8d5c56151e41f28b3e0a6ad183b1c3","placeholder":"​","style":"IPY_MODEL_8fb7cdd7f23d4de480889c322182cc32","value":"Downloading adapter_model.bin: 100%"}},"d82ae21155e24950bf2f200832869c20":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_79a76b2bcfff4f5899a0b0a0b3d928c6","max":101834237,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5014544610934dbb9a156a0d0a0d3684","value":101834237}},"2b360333c9da405ea70cd559c2bd0fe4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0be0fc8a6f0b477a9c76aeb4dd1f89a5","placeholder":"​","style":"IPY_MODEL_209e1bf7fab54e53afff7567884fca10","value":" 102M/102M [00:01&lt;00:00, 88.2MB/s]"}},"3b75ab9513b347daa0bdb01550d386f4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4e8d5c56151e41f28b3e0a6ad183b1c3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8fb7cdd7f23d4de480889c322182cc32":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"79a76b2bcfff4f5899a0b0a0b3d928c6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5014544610934dbb9a156a0d0a0d3684":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0be0fc8a6f0b477a9c76aeb4dd1f89a5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"209e1bf7fab54e53afff7567884fca10":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#1.Enter Repo Name and ID\nHUGGING_FACE_USER_NAME = \"Tatvajsh\"\nmodel_name = \"AHS_OPS_alpha_v_2.0\"","metadata":{"id":"ulSGQGOldJmK","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install transformers[sentencepiece]\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0PcIvk0eeHpU","outputId":"c22d304e-09ac-4b8e-86f5-17d1dcd05b52"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#2.Install Necessary Libraries\n!pip install -q bitsandbytes datasets accelerate loralib\n!pip install -q git+https://github.com/huggingface/peft.git git+https://github.com/huggingface/transformers.git","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ShT-MzfjdJmP","outputId":"d7e186fe-b68a-4b03-e148-d696d96a889a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#3.Load the Model with Adapter in GPU for INFERENCE\nimport torch\nfrom peft import PeftModel, PeftConfig\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\npeft_model_id = f\"{HUGGING_FACE_USER_NAME}/{model_name}\"\nconfig = PeftConfig.from_pretrained(peft_model_id)\nmodel = AutoModelForCausalLM.from_pretrained(\"openlm-research/open_llama_3b_v2\", device_map=\"auto\",load_in_4bit=True)\n\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["f24e43907ddf4843b1882e2ebced166e","8d8649dd03e54f3281ebe45d3969d6f5","6e736b806e684fc5a1c79dea6195c487","9a995ed9556f4682bf492e5debc92e54","2148ff4c7d6a4ab48bcbe0cce7311fed","0c462a91ef2141f792e4cfabff176049","be5ccd3fa5394cc6b2d1693a31e917f5","24e440d5d3ad4e28ae0152dd7f66d785","cd8be2fecc5f4ada96e00a5ad6e9aafb","44aeb868743e466ea90e1d3748f878bd","70f3facc0bf344d788d1eca8fddcb08d"]},"id":"HwzOFejAdJmQ","outputId":"de82d553-30c3-47ab-8cea-22a54b6ad540","jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(peft_model_id,use_fast=False)\n# Load the Lora model\nmodel1 = PeftModel.from_pretrained(model, peft_model_id,device_map=\"auto\")\nmodel1.resize_token_embeddings(len(tokenizer))\nmodel1","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["e68425f516ea474993dc28e50dd2818f","aed4fd1d25c54f89a3b9385daf8ed643","d82ae21155e24950bf2f200832869c20","2b360333c9da405ea70cd559c2bd0fe4","3b75ab9513b347daa0bdb01550d386f4","4e8d5c56151e41f28b3e0a6ad183b1c3","8fb7cdd7f23d4de480889c322182cc32","79a76b2bcfff4f5899a0b0a0b3d928c6","5014544610934dbb9a156a0d0a0d3684","0be0fc8a6f0b477a9c76aeb4dd1f89a5","209e1bf7fab54e53afff7567884fca10"]},"id":"3w7RDWWKdJmS","outputId":"7868d7df-477d-4a56-fe2b-ab19ad9adb03","jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nprompt1=f\"\"\"\n Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n    ### Instruction:\n\n    the URL for the official website of the AHS Health Link Policies department is https://insite.albertahealthservices.ca/811/Page8024.aspx\n\n    ### Response:\n\n    \"\"\"\ntokenizer.tokenize(prompt1, return_tensors='pt',add_special_tokens=True)\n# tokenizer","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_SctL5ICnxLl","outputId":"8d10ca5a-0615-45e7-999e-981d4be2b2a7","jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# BEST HYPERPARAMETERS FOR NOW:    \n\n> outputs = model.generate(input_ids=batch[\"input_ids\"].to(\"cuda\"),\n                            temperature=0.001,\n                             max_new_tokens=150,\n                             repetition_penalty=1.2,\n                             output_scores=True,\n                             top_k=90,\n                             do_sample=True)\n","metadata":{"id":"cCpN040UdJmT"}},{"cell_type":"code","source":"#4.Generate the Text based upon user Question\nfrom IPython.display import display, Markdown\n\ndef make_inference(instructions):\n\n  prompt = f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\\n    ### Instruction:{instructions}  \\\n    ### Response:  \"\"\"\n  prompt1=f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n    ### Instruction:\n    {instructions}\n    ### Response:\n    \"\"\"\n  prompt2=f\"Q:{instructions} \\n A:\"\n  batch = tokenizer(prompt1, return_tensors='pt')\n#   model.config.use_cache = True\n  with torch.no_grad():\n    outputs = model1.generate(input_ids=batch[\"input_ids\"].to(\"cuda\"),use_cache=False,output_attentions=False,min_new_tokens=60,temperature=0.2,max_new_tokens=512,repetition_penalty=1.2,output_scores=True,return_dict_in_generate=True,top_p=0.3,top_k=50,do_sample=True)\n      # outputs = model1.generate(input_ids=batch[\"input_ids\"].to(\"cuda\"),use_cache=True,max_new_tokens=512,repetition_penalty=1.2,output_scores=True,return_dict_in_generate=True)\n\n    # print(tokenizer.batch_decode(outputs.detach().cpu().numpy())[0])\n  return tokenizer.batch_decode(outputs[0].detach().cpu().numpy(),skip_special_tokens=False)[0],outputs\n  # display(Markdown(tokenizer.decode(output[0], skip_special_tokens=True)))","metadata":{"id":"3UpsdigddJmV","jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport time\nstart_time = time.time()\ninstruction=\" What does Operations oversee?\"\n\nprompt=f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n    ### Instruction:\n    {instruction}\n    ### Response:\n    \"\"\"\nresp,outputs=make_inference(f\"{instruction}\")\nend_time = time.time()\n# Calculate the elapsed time\nelapsed_time = end_time - start_time\n\nprint(f\"Execution time: {elapsed_time} seconds\")\nprint(resp)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LwnQT5X_dJmX","outputId":"fbdc6e3a-2dee-4aa3-867d-fec656bbefb5","jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n\n\n#6.OUTPUT FORMAT 1\nanswer=\" \".join(resp.split(\"Response:\")[1:])\nprint(answer)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cFM7_epIdJmZ","outputId":"0c11b1e1-8c14-48fd-905c-c54b66496cc3","jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nimport numpy as np\ntransition_scores = model.compute_transition_scores(\n    outputs.sequences, outputs.scores, normalize_logits=True\n)\ntransition_scores\ninputs=tokenizer(f\"{prompt}\", return_tensors='pt')\ninput_length = 1 if model.config.is_encoder_decoder else inputs.input_ids.shape[1]\ngenerated_tokens = outputs.sequences[:, input_length:]\nfor tok, score in zip(generated_tokens[0], transition_scores[0]):\n    # | token | token string | logits | probability\n    print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.cpu().data.numpy():.3f} | {np.exp(score.cpu().data.numpy()):.2%}\")","metadata":{"id":"nV7FVr9Dh98G","colab":{"base_uri":"https://localhost:8080/"},"outputId":"64262dd1-8700-4e7d-933c-c9de97fac95d","jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip -q install langchain huggingface_hub transformers sentence_transformers faiss-gpu accelerate\n!pip install -q bitsandbytes datasets accelerate loralib\n!pip install -q git+https://github.com/huggingface/peft.git git+https://github.com/huggingface/transformers.git","metadata":{"id":"js1MHzy3-fvr","execution":{"iopub.status.busy":"2023-10-30T20:53:10.394415Z","iopub.execute_input":"2023-10-30T20:53:10.394691Z","iopub.status.idle":"2023-10-30T20:54:34.464858Z","shell.execute_reply.started":"2023-10-30T20:53:10.394668Z","shell.execute_reply":"2023-10-30T20:54:34.463645Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.document_loaders import PyPDFLoader,DirectoryLoader,TextLoader,CSVLoader\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.vectorstores import FAISS","metadata":{"id":"23wuEZ71dJmc","execution":{"iopub.status.busy":"2023-10-30T20:54:34.466380Z","iopub.execute_input":"2023-10-30T20:54:34.466747Z","iopub.status.idle":"2023-10-30T20:54:35.504147Z","shell.execute_reply.started":"2023-10-30T20:54:34.466715Z","shell.execute_reply":"2023-10-30T20:54:35.503316Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\nmodel = AutoModelForCausalLM.from_pretrained(\n        \"openlm-research/open_llama_3b_v2\",\n        device_map=\"auto\",\n        torch_dtype=torch.float16,\n        load_in_4bit=True\n#         low_cpu_mem_usage=True\n#         load_in_8bit=True,#original for best performance:        load_in_4bit=True\n\n    )\ntokenizer = AutoTokenizer.from_pretrained(\"openlm-research/open_llama_3b_v2\",trust_remote_code=True,use_fast=False,padding=256,truncation=256)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mXdzbM5VGDdI","outputId":"91140c7a-3aa2-442b-f6dc-37889cfb801b","execution":{"iopub.status.busy":"2023-10-30T20:54:35.506449Z","iopub.execute_input":"2023-10-30T20:54:35.506877Z","iopub.status.idle":"2023-10-30T20:55:57.185177Z","shell.execute_reply.started":"2023-10-30T20:54:35.506848Z","shell.execute_reply":"2023-10-30T20:55:57.184256Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/506 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52312101665341d9bb6e9837c8e16415"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/6.85G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da9e70ca2eb8407088a0fbdc1fad445f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)neration_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9c958c35c764234a0f720f6e2c903ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/593 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06b8d7db99cd4a7fa32daad9f5431600"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer.model:   0%|          | 0.00/512k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d46a3645fd834bdfb87c78ba2b812004"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/330 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6a10643488f4895867a0465995c4140"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","output_type":"stream"}]},{"cell_type":"code","source":"#1.Enter Repo Name and ID\nHUGGING_FACE_USER_NAME = \"Tatvajsh\"\nmodel_name1 = \"AHS_OPS_alpha_v_2.0\"\nmodel_name2=\"AHS_WPCS_alpha_v_1.0\"\npeft_model_id = f\"{HUGGING_FACE_USER_NAME}/{model_name1}\"","metadata":{"id":"NFYblMosoDVj","execution":{"iopub.status.busy":"2023-10-30T20:55:57.186272Z","iopub.execute_input":"2023-10-30T20:55:57.186553Z","iopub.status.idle":"2023-10-30T20:55:57.191361Z","shell.execute_reply.started":"2023-10-30T20:55:57.186527Z","shell.execute_reply":"2023-10-30T20:55:57.190413Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from peft import PeftModel, PeftConfig\nmodel1 = PeftModel.from_pretrained(model, peft_model_id,device_map=\"auto\",torch_dtype=torch.float16, low_cpu_mem_usage=True)\n# merged_model = model1.merge_and_unload()\n# model1.load_adapter(peft_model_id, adapter_name=\"wpcs\")\n# model2 = PeftModel.from_pretrained(model, peft_model_id,device_map=\"auto\",adapter_name=\"ops\")\n","metadata":{"id":"a9QIYtPsoB7v","execution":{"iopub.status.busy":"2023-10-30T20:55:57.192609Z","iopub.execute_input":"2023-10-30T20:55:57.192979Z","iopub.status.idle":"2023-10-30T20:56:00.044152Z","shell.execute_reply.started":"2023-10-30T20:55:57.192946Z","shell.execute_reply":"2023-10-30T20:56:00.043021Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)/adapter_config.json:   0%|          | 0.00/594 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9145b3ac51014c48b7240a173a23e245"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading adapter_model.bin:   0%|          | 0.00/51.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7bb0b4ca9e4e46369f30eb9dec729c9c"}},"metadata":{}}]},{"cell_type":"code","source":"model1.resize_token_embeddings(len(tokenizer))","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model1","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from peft import PeftModel, PeftConfig\nmodel2 = PeftModel.from_pretrained(model, peft_model_id,device_map=\"auto\",adapter_name=\"ops\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model2\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model1.resize_token_embeddings(len(tokenizer))","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model1","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipe = pipeline(\"text-generation\",\n                model=model1,\n                tokenizer= tokenizer,\n                torch_dtype=torch.bfloat16,\n                device_map=\"auto\",\n                max_new_tokens = 256,\n                do_sample=True,\n                top_k=30,\n                num_return_sequences=2,\n                temperature=0.50,\n                # eos_token_id=tokenizer.eos_token_id,\n                # use_cache=True,\n                repetition_penalty=1.1,\n                # output_scores=True,\n                # return_dict_in_generate=True,\n                # num_beams=15,\n                )","metadata":{"execution":{"iopub.status.busy":"2023-10-30T18:55:47.307593Z","iopub.execute_input":"2023-10-30T18:55:47.307956Z","iopub.status.idle":"2023-10-30T18:55:47.315062Z","shell.execute_reply.started":"2023-10-30T18:55:47.307919Z","shell.execute_reply":"2023-10-30T18:55:47.313956Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n","output_type":"stream"}]},{"cell_type":"code","source":"DATA_PATH=\"/kaggle/input/latest-dataset\"\nDB_FAISS_PATH=\"/kaggle/working/vectorstores/db_faiss\"\n\n#create vector database\ndef create_vector_db():\n    #Create a DirectoryLoader to load all PDFs from the DATA_PATH. Use the PyPDFLoader to load each PDF.\n    loader=DirectoryLoader(DATA_PATH,glob=\"*.txt\",loader_cls=TextLoader)\n    documents=loader.load()\n    #shared overlapping text gives some continuity between chunks and context.\n    text_splitter=RecursiveCharacterTextSplitter(chunk_size=400,chunk_overlap=50)\n    # text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=150)\n\n    texts=text_splitter.split_documents(documents)# all the splitted text is here,text chunks\n    embeddings=HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n                                     model_kwargs={'device':'cuda'})#creating the embeddings\n    db=FAISS.from_documents(texts,embeddings)#using this embedding model,create all the embedding and store it\n    db.save_local(DB_FAISS_PATH)\n\n\ncreate_vector_db()","metadata":{"id":"4LESzdiylxtD","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n                                       model_kwargs={'device': 'cuda'})\n\ndb = FAISS.load_local(\"/kaggle/working/vectorstores/db_faiss\", embeddings)","metadata":{"id":"xZx6VBaSkJ0Y","execution":{"iopub.status.busy":"2023-10-30T18:56:03.124550Z","iopub.execute_input":"2023-10-30T18:56:03.125693Z","iopub.status.idle":"2023-10-30T18:56:03.716624Z","shell.execute_reply.started":"2023-10-30T18:56:03.125656Z","shell.execute_reply":"2023-10-30T18:56:03.715600Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from langchain import HuggingFacePipeline\nfrom langchain import PromptTemplate,  LLMChain\n# from langchain.document_loaders import PyPDFLoader, DirectoryLoader\nfrom langchain import PromptTemplate\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.vectorstores import FAISS\nfrom langchain.llms import CTransformers\nfrom langchain.chains import RetrievalQA\nllm = HuggingFacePipeline(pipeline = pipe, model_kwargs = {'temperature':0.30,'repetition_penalty':1.3,\"num_beams\":10})","metadata":{"id":"k78IPrfklV-d","execution":{"iopub.status.busy":"2023-10-30T18:56:04.233478Z","iopub.execute_input":"2023-10-30T18:56:04.234197Z","iopub.status.idle":"2023-10-30T18:56:05.350435Z","shell.execute_reply.started":"2023-10-30T18:56:04.234160Z","shell.execute_reply":"2023-10-30T18:56:05.349580Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"custom_prompt_template=\"\"\"Given the context information provided, please answer the user's question.\n\nContext: {context}\n\nQuestion: {question}\n\"\"\"\ncustom_prompt_template1=\"\"\"\nGiven the context information provided, please concisely and accurately answer the user's question. If the provided context is insufficient to determine an answer, please say so. Where appropriate, please provide any explanations or additional details that would be helpful to understanding the answer. Please do not speculate or fabricate information beyond what can be inferred from the given context.\n\n### Context: {context}\n\n### Question: {question}\n### Answer: \"\"\"\ncustom_prompt_template2=\"\"\"\n Below is an instruction that describes a task. Write a response that appropriately completes the request from the given context\n    ### Instruction:\n    {question}\n    ### Context:\n    {context}\n    ### Response:\n\n    \"\"\"\ncustom_prompt_template3=\"\"\"\n Below is an instruction that describes a task. Write a response that appropriately completes the request from thr given context\n    ### Instruction=>\n    {question}\n    ### Context=>\n    {context}\n    ### Response=>\n\n    \"\"\"\n#concisely while including all relevant details. Reference any links, phone numbers, or email addresses that are relevant to the answer verbatim. If the question cannot be fully answered based on the given context, respond \"I do not have enough context to fully answer this question. Please rephrase or provide more details about what you are looking for.\" Do not speculate or make up an answer.\n# Concise, factual answer using relevant details and verbatim references from the provided context:\nprompt = PromptTemplate(template=custom_prompt_template2,input_variables=[ 'question',\"context\"])\nqa_chain = RetrievalQA.from_chain_type(llm=llm,\n                                       chain_type='stuff',\n                                       retriever=db.as_retriever(search_kwargs={'k': 3}),\n                                       return_source_documents=True,\n                                       chain_type_kwargs={'prompt': prompt}\n                                       )","metadata":{"id":"dLsGjJ0yldYF","execution":{"iopub.status.busy":"2023-10-30T18:56:05.351990Z","iopub.execute_input":"2023-10-30T18:56:05.352278Z","iopub.status.idle":"2023-10-30T18:56:05.360915Z","shell.execute_reply.started":"2023-10-30T18:56:05.352251Z","shell.execute_reply":"2023-10-30T18:56:05.359897Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"question = \"Where can I find COVID-19 call handling documents?\"\n# question = \"Where  the Manager on Call (MOC) Addiction and Mental Health Helpline can I find?\"\n\nfrom langchain import PromptTemplate,  LLMChain\nres=qa_chain({'query': question})\n# res=qa_chain.run(question)\nprint(res[\"result\"])#Navigation Resources include Provincial Manager, Coordinators, a Web Master, and Clerical Support Staff. They maintain clinical and non-clinical content such as flowcharts, RightFax, EchoAccess, the Health Link intranet, and the Inform Alberta website. [Navigation Resources (https://insite.albertahealthservices.ca/811/Page15109.aspx)]","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nXhRCykaldbW","outputId":"91fdd092-afbf-478c-d515-8ece1b9f56c7","execution":{"iopub.status.busy":"2023-10-30T18:56:06.043512Z","iopub.execute_input":"2023-10-30T18:56:06.043906Z","iopub.status.idle":"2023-10-30T18:56:56.615924Z","shell.execute_reply.started":"2023-10-30T18:56:06.043876Z","shell.execute_reply":"2023-10-30T18:56:56.614721Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72a2df14eb764db192ec8826a484b65c"}},"metadata":{}},{"name":"stdout","text":"COVID-19 call handling documents can be found [here](https://insite.albertahealthservices.ca/811/Page24484.aspx#doc-update).\nThe process for COVID vaccine escalation for people who are homebound can be found [here](https://share.albertahealthservices.ca/teams/HLA/Webdocs/Emerging%20Events/tms-811-covid-vaccine-escalation-homebound.pdf).\nWork instruction documents related to COVID-19 can be found [here](https://insite.albertahealthservices.ca/811/Page24685.aspx#work-instruction-docs).\nTesting protocols are available [here](https://insite.albertahealthservices.ca/811/Page24685.aspx#testing-protocols).\nWork instruction documents related to COVID-19 testing can be found [here](https://insite.albertahealthservices.ca/811/Page24685.aspx#test-work-\n","output_type":"stream"}]},{"cell_type":"code","source":"res\n#  The IT Acceptable Use Policy at AHS provides guidance and standards on the use of IT resources, including the Internet and electronic forms of communication (e-mail) for business purposes. It emphasizes the need for exercising careful judgment when using the Internet, intranet, e-mail, or other AHS IT resources.\n# The URL for the Insulin Pump Therapy learning module is [here](https://myhealth.alberta.ca/learning/modules/insulin-pump-therapy).\n# You can find the 'Collection, Access, Use, & Disclosure of Information 1112' policy [here](https://extranet.ahsnet.ca/teams/policydocuments/1/clp-ahs-pol-collection-access-use-disclosure-information.pdf).","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zwiELMG1IOJW","outputId":"3fca5f87-042c-46e4-ad3f-e37f55a5be9d","execution":{"iopub.status.busy":"2023-10-30T18:56:56.617546Z","iopub.execute_input":"2023-10-30T18:56:56.617877Z","iopub.status.idle":"2023-10-30T18:56:56.625543Z","shell.execute_reply.started":"2023-10-30T18:56:56.617848Z","shell.execute_reply":"2023-10-30T18:56:56.624251Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"{'query': 'Where can I find COVID-19 call handling documents?',\n 'result': 'COVID-19 call handling documents can be found [here](https://insite.albertahealthservices.ca/811/Page24484.aspx#doc-update).\\nThe process for COVID vaccine escalation for people who are homebound can be found [here](https://share.albertahealthservices.ca/teams/HLA/Webdocs/Emerging%20Events/tms-811-covid-vaccine-escalation-homebound.pdf).\\nWork instruction documents related to COVID-19 can be found [here](https://insite.albertahealthservices.ca/811/Page24685.aspx#work-instruction-docs).\\nTesting protocols are available [here](https://insite.albertahealthservices.ca/811/Page24685.aspx#testing-protocols).\\nWork instruction documents related to COVID-19 testing can be found [here](https://insite.albertahealthservices.ca/811/Page24685.aspx#test-work-',\n 'source_documents': [Document(page_content='COVID-19 call handling documents can be found [here](https://insite.albertahealthservices.ca/811/Page24484.aspx#doc-update).\\nYou can find the process for COVID vaccine escalation for people who are homebound [here](https://share.albertahealthservices.ca/teams/HLA/Webdocs/Emerging%20Events/tms-811-covid-vaccine-escalation-homebound.pdf).', metadata={'source': '/kaggle/input/latest-dataset/combined_2.txt'}),\n  Document(page_content=\"Work instruction documents related to COVID-19 can be found [here](https://insite.albertahealthservices.ca/811/Page24685.aspx#work-instruction-docs).\\nYou can stay informed about the latest news and updates within Health Link Alberta by regularly checking the 'Health Link News & Updates' section on their platform.\", metadata={'source': '/kaggle/input/latest-dataset/combined_2.txt'}),\n  Document(page_content=\"COVID-19 testing protocols are available [here](https://insite.albertahealthservices.ca/811/Page24685.aspx#testing-protocols).\\nCOVID-19 work instruction documents are accessible [here](https://insite.albertahealthservices.ca/811/Page24685.aspx#work-instruction-docs).\\nNews and updates related to Health Link Alberta can be found under 'Health Link News & Updates'.\", metadata={'source': '/kaggle/input/latest-dataset/combined_2.txt'})]}"},"metadata":{}}]},{"cell_type":"code","source":"merged_model = model1.merge_and_unload()\n\nfrom huggingface_hub import notebook_login\n\nnotebook_login()\n\nmerged_model.push_to_hub(\"Tatvajsh/AHS_OPS_GGUF\")","metadata":{"execution":{"iopub.status.busy":"2023-10-30T20:56:59.190938Z","iopub.execute_input":"2023-10-30T20:56:59.191751Z","iopub.status.idle":"2023-10-30T20:59:41.223268Z","shell.execute_reply.started":"2023-10-30T20:56:59.191719Z","shell.execute_reply":"2023-10-30T20:59:41.222164Z"},"trusted":true},"execution_count":11,"outputs":[]}]}